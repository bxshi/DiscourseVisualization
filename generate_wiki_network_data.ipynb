{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw SQL data from Wikipedia Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From enwiki-20171001-category.sql\n",
    "# cat enwiki-20171001-category.sql | python tocsv.py > enwiki-20171001-category.csv\n",
    "category = pd.read_csv('./enwiki-20171001-category.csv', header=None, sep=',', encoding='latin1')\n",
    "category.columns = ['cat_id', 'cat_title', 'cat_page', 'cat_subcats', 'cat_files']\n",
    "# From enwiki-20171001-page.sql\n",
    "# cat enwiki-20171001-page.sql | python tocsv.py > enwiki-20171001-page.csv\n",
    "page = pd.read_csv('./enwiki-20171001-page.csv', header=None, sep=',', usecols=[0, 1, 2, 5, 12, 13], encoding='latin1')\n",
    "page.columns = ['page_id', 'page_namespace', 'page_title',\n",
    "                'page_is_redirect', 'page_content_model', 'page_lang']\n",
    "# From enwiki-20171001-categorylinks.sql\n",
    "# cat enwiki-20171001-categorylinks.sql | python tocsv.py --ignore_column 2 --ignore_column 3 --ignore_column 4 --ignore_column 5 > enwiki-20171001-categorylinks.csv\n",
    "categorylinks = pd.read_csv('./enwiki-20171001-categorylinks.csv', header=None, sep=',', encoding='latin1')\n",
    "categorylinks.columns = ['cl_from', 'cl_to', 'cl_type']\n",
    "# cat enwiki-20171001-templatelinks.sql | python tocsv.py > enwiki-20171001-templatelinks.csv\n",
    "templatelinks = pd.read_csv('./enwiki-20171001-templatelinks.csv', header=None, sep=',', encoding='latin1')\n",
    "templatelinks.columns = ['tl_from', 'tl_namespace', 'tl_title', 'tl_from_namespace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templatelinks[templatelinks.tl_title.apply(lambda x: str(x).startswith('Tracking'))].tl_title.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up categories\n",
    "\n",
    "According to https://en.wikipedia.org/wiki/Wikipedia:Categorization, there are a few non-content categories with the following prefixes:\n",
    "\n",
    "* `Wikipedia`\n",
    "* `WikiProject`\n",
    "\n",
    "Besides these, we also need to remove categories with template `Wikipedia_category` and `Maintenance_category` and its descendants according to https://en.wikipedia.org/wiki/Template:Maintenance_category. Note that `Template:Wikipedia_category` redirects to the same template page.\n",
    "\n",
    "So here we will first find all templates inherit from these two category templates, and then remove all categories using these templates. \n",
    "\n",
    "Then we will check if any category with the abovementioned prefix still exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all categories that use `Wikipedia Category` and `Maintenance Category` as templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all non-template elements are removed\n",
      "find 48 new non-content template\n",
      "find 31 new non-content template\n",
      "find 8 new non-content template\n",
      "find 0 new non-content template\n",
      "In total we find 61 wikipedia templates.\n",
      "In total we find 52646 wikipedia categories.\n"
     ]
    }
   ],
   "source": [
    "def find_wikipedia_category_templates(templatelink_df, page_df):\n",
    "    # Two basic templates\n",
    "    selected_templates = set(['Wikipedia_category', 'Maintenance_category', 'Tracking_category', 'Trackingcat', 'Tracking_category/doc', 'Tracking_cat'])\n",
    "    # Do BFS here to find all templates using these two root templates\n",
    "    curr_parent_templates = selected_templates.copy()\n",
    "    template_page_df = page_df[page_df.page_namespace == 10]  # limit namespace to template\n",
    "    template_df = templatelink_df[templatelink_df.tl_from_namespace == 10]  # limit namespace to template\n",
    "    print(\"all non-template elements are removed\")\n",
    "    while len(curr_parent_templates):\n",
    "        # all template ids inherit from curr_parent_templates\n",
    "        next_template_ids = template_df[template_df.tl_title.isin(curr_parent_templates)].tl_from.unique()\n",
    "        next_templates = template_page_df[template_page_df.page_id.isin(next_template_ids)].page_title.unique()\n",
    "        selected_templates |= curr_parent_templates\n",
    "        # Remove seen templates\n",
    "        template_df = template_df[~template_df.tl_title.isin(curr_parent_templates)]\n",
    "        # update template\n",
    "        curr_parent_templates = set(next_templates)\n",
    "        print(\"find %d new non-content template\" % len(curr_parent_templates))\n",
    "    print(\"In total we find %d wikipedia templates.\" % len(selected_templates))\n",
    "    cate_template_df = templatelink_df[templatelink_df.tl_from_namespace == 14]\n",
    "    wiki_category = cate_template_df[cate_template_df.tl_title.isin(selected_templates)].tl_from\n",
    "    print(\"In total we find %d wikipedia categories.\" % len(wiki_category))\n",
    "    content_category = page_df[page_df.page_namespace==14]\n",
    "    content_category = content_category[~content_category.page_id.isin(wiki_category)]\n",
    "    return content_category[['page_id', 'page_title']]\n",
    "    \n",
    "content_categories = find_wikipedia_category_templates(templatelinks, page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove other categories\n",
    "\n",
    "* Year related categories `1990s_animated_films`, `Conflicts_in_1954`, `October_2017_events`, `1990_deaths`, ...\n",
    "* Other remaining categories with name `Wikipedia` or `WikiProject`\n",
    "* Other remaining stub categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing all year related categories, there are 1305682 categories left.\n",
      "After removing all Wikipedia related categories, there are 1252749 categories left.\n",
      "After removing all stub categories, there are 1235610 categories left.\n",
      "After removing all *by*/*in* categories, there are 1140758 categories left.\n",
      "After removing all 4-digit years, there are 1117524 categories left.\n",
      "After removing template and archive categories, there are 1115479 categories left.\n"
     ]
    }
   ],
   "source": [
    "# Remove all categories that grouped by years\n",
    "YEAR_REGEX=re.compile(r'^\\d+(s|AD|BC)?\\_(birth|deaths|establishments|disestablishments|books|by|events|in|km|works|conflicts|elections)')\n",
    "YEAR_REGEX_2=re.compile(r'^\\d{4}(s)?\\_')  # 1990s_animated_films\n",
    "YEAR_REGEX_3=re.compile(r'.*\\_\\d{4}$')  # Conflicts_in_1954\n",
    "# October_2017_events\n",
    "YEAR_REGEX_4=re.compile(r'^(January|February|March|April|May|June|July|August|September|October|November|December)\\_\\d{4}')\n",
    "content_categories = content_categories[content_categories.page_title.apply(lambda x: YEAR_REGEX.match(x) is None and YEAR_REGEX_2.match(x) is None and YEAR_REGEX_3.match(x) is None and YEAR_REGEX_4.match(x) is None)]\n",
    "print(\"After removing all year related categories, there are %d categories left.\" % len(content_categories))\n",
    "\n",
    "# Remove all categories starts with Wikipedia\n",
    "WIKI_REGEX=re.compile(r'^(wikipedia|wikiproject)(\\_|\\-)')\n",
    "content_categories = content_categories[content_categories.page_title.apply(lambda x: WIKI_REGEX.match(x.lower()) is None and ('wikipedia' not in x.lower()) and ('wikiproject' not in x.lower()))]\n",
    "print(\"After removing all Wikipedia related categories, there are %d categories left.\" % len(content_categories))\n",
    "\n",
    "# Remove stub categories and container categories\n",
    "content_categories = content_categories[content_categories.page_title.apply(lambda x: not x.lower().startswith('stub_categories') and not x.lower().startswith('container_categories') and ('stub' not in x.lower()))]\n",
    "print(\"After removing all stub categories, there are %d categories left.\" % len(content_categories))\n",
    "\n",
    "# Remove something_by_something cateogires, e.g. \n",
    "content_categories = content_categories[content_categories.page_title.apply(lambda x: '_by_' not in x.lower() )]# and '_in_' not in x.lower())]\n",
    "print(\"After removing all *by*/*in* categories, there are %d categories left.\" % len(content_categories))\n",
    "\n",
    "# Remove chronology\n",
    "DIGIT=re.compile('(?<=[^\\(])\\d{4}')\n",
    "DIGIT2=re.compile('^\\d+(s)?$')\n",
    "DIGIT3=re.compile('^\\d+_BC$')\n",
    "# def digit_validation(x):\n",
    "#     digs = DIGIT.findall(str(x))\n",
    "#     return len(digs) == 1 and (int(digs[0]) <= 2020) and (int(digs[0]) >= 1000)\n",
    "content_categories = content_categories[content_categories.page_title.apply(lambda x: len(DIGIT.findall(x)) == 0 and len(DIGIT2.findall(x)) == 0 and (DIGIT3.match(x) is None) and x != 'Years' and x != 'Decades')]\n",
    "print(\"After removing all 4-digit years, there are %d categories left.\" % len(content_categories))\n",
    "\n",
    "# Remove template and archive links\n",
    "content_categories = content_categories[content_categories.page_title.apply(lambda x: '_template_' not in str(x).lower() and 'webarchive' not in str(x).lower() and 'all_articles' not in str(x).lower() and 'external_links' not in str(x).lower() and 'cs1_errors' not in x.lower() and not x.startswith('CS1_') and not x.startswith('Articles_') and not x.startswith('All_') and 'disambiguation' not in x.lower())]\n",
    "print(\"After removing template and archive categories, there are %d categories left.\" % len(content_categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate category network\n",
    "After we cleaned up categories, now we construct the category network using the `categorylinks`. We will need to first extract category_to_category edges first, then convert page_id (`tl_from`) into titles, and then only keep the ones within our `categorylinks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_links = categorylinks[categorylinks.cl_type == 'subcat'] # only cate-to-cate links\n",
    "cate_page = page[page.page_namespace == 14]  # only cates are source\n",
    "# convert category ids into category names\n",
    "cate_links = cate_links.merge(page, left_on='cl_from', right_on='page_id')[['page_title', 'cl_to']]\n",
    "cate_links.columns = ['cl_from', 'cl_to']\n",
    "content_category_set = set(content_categories.page_title.values)  # set of valid encyclopedia categories\n",
    "# Get cate-cate links within valid encyclopedia categories\n",
    "cate_links = cate_links[(cate_links.cl_from.isin(content_category_set)) & (cate_links.cl_to.isin(content_category_set))]\n",
    "\n",
    "# Reindex all selected categories, output the new index into a file\n",
    "content_category_id_pd = pd.DataFrame({'title':list(content_category_set), 'title_id':np.arange(len(content_category_set))})\n",
    "content_category_id_pd.to_csv('./data/content_category_id.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `Networkx` category-to-category network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "digraph = nx.DiGraph()\n",
    "digraph.add_edges_from(cate_links.values)  # create graph using the filtered category-to-category links\n",
    "revgraph = digraph.reverse()  # reversed graph, from large-topic -> small-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest main topic ancestor for all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_main_topic_ancestor(g):\n",
    "    parents = dict()\n",
    "    for root_node in g.neighbors('Main_topic_classifications'):\n",
    "        if root_node not in parents:\n",
    "            parents[root_node] = dict()\n",
    "        parents[root_node][root_node] = 0\n",
    "        res = nx.algorithms.shortest_paths.unweighted.single_source_shortest_path_length(g, root_node, cutoff=50)\n",
    "        for k,v in res.items():\n",
    "            if k not in parents:\n",
    "                parents[k] = dict()\n",
    "            parents[k][root_node] = v\n",
    "    return parents\n",
    "\n",
    "parents = find_nearest_main_topic_ancestor(revgraph)\n",
    "del revgraph\n",
    "# Convert result into a pandas dataframe\n",
    "nearest_parent = {k:min(v, key=v.get) for k,v in parents.items()}\n",
    "nearest_parent_pd = pd.DataFrame({'title':list(nearest_parent.keys()), 'label':list(nearest_parent.values())})\n",
    "\n",
    "# Assign labels to the nodes in the graph\n",
    "for node in digraph.nodes():\n",
    "    try:\n",
    "        lbl = nearest_parent[node]\n",
    "    except KeyError:\n",
    "        lbl = 'Unknown'\n",
    "    digraph.nodes[node]['label'] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save NetworkX\n",
    "nx.write_gpickle(digraph, './data/category_network.nx.pickle')\n",
    "\n",
    "# Save the nearest main topic ancestor into a file\n",
    "# Generate links between individual category and their nearest root category\n",
    "# cate_to_root = nearest_parent_pd.merge(content_category_id_pd, left_on='label', right_on='title')[['title_x', 'title_id']]\n",
    "# cate_to_root.columns = ['cl_from', 'cl_to_id']\n",
    "# cate_to_root = cate_to_root.merge(content_category_id_pd, left_on='cl_from', right_on='title')[['title_id', 'cl_to_id']]\n",
    "# cate_to_root.columns = ['cl_from_id', 'cl_to_id']\n",
    "# cate_to_root.to_csv('./category_network.root.txt', sep=' ', index=False)\n",
    "\n",
    "# output main topic colors\n",
    "tableau_colors = ['31,119,180', '174,199,232', '255,127,14', '255,187,120', '44,160,44',\n",
    "                  '152,223,138', '214,39,40', '255,152,150', '148,103,189', '197,176,213',\n",
    "                  '140,86,75', '196,156,148', '227,119,194', '247,182,210', '127,127,127',\n",
    "                  '199,199,199', '188,189,34', '219,219,141', '23,190,207', '158,218,229',\n",
    "                  '65,68,81', '159,205,153']\n",
    "\n",
    "parent_color = {k:v for k,v in zip(sorted(set([str(x) for x in nearest_parent.values()])), tableau_colors)}\n",
    "parent_ids = {k:v for v,k in enumerate(parent_color.keys())}\n",
    "# nearest_parent_id_pd['label_id'] = nearest_parent_id_pd.label.apply(lambda x: parent_ids[x])\n",
    "# nearest_parent_id_pd[['title_id', 'label_id']].to_csv('./data/category_label.id.txt', sep=' ', index=False)\n",
    "\n",
    "with open('./data/category_label.color.txt', 'w') as fout:\n",
    "    for k,v in parent_color.items():\n",
    "        r,g,b = [int(x) for x in v.split(',')]\n",
    "        fout.write('%s %d %d %d\\n' % (k, r,g,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find page and their categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all page -> ??? links\n",
    "page_links = categorylinks[categorylinks.cl_type == 'page']\n",
    "\n",
    "# Find all page -> category\n",
    "selected_pages = page_links[page_links.cl_to.isin(content_category_set)].merge(page[page.page_namespace == 0][['page_id', 'page_title']], left_on='cl_from', right_on='page_id')[['page_title', 'cl_to']]\n",
    "selected_pages.columns = ['cl_from', 'cl_to']\n",
    "selected_pages=selected_pages.merge(content_category_id_pd, left_on='cl_to', right_on='title')[['cl_from', 'cl_to', 'title_id']]\n",
    "selected_pages.columns=['cl_from', 'cl_to', 'cl_to_id']\n",
    "# Add redirection pages\n",
    "redirection = pd.read_csv('../enwiki-20171001-redirect.csv', header=None, sep=',', encoding='latin1', usecols=[0,1,2])\n",
    "redirection.columns = ['rd_from', 'rd_namespace', 'rd_title']\n",
    "redirect = redirection[redirection.rd_namespace == 0].merge(page, left_on='rd_from', right_on='page_id')[['page_title', 'rd_title']]\n",
    "redirect = redirect.merge(selected_pages, left_on='rd_title', right_on='cl_from')[['page_title', 'cl_to', 'cl_to_id']]\n",
    "redirect.columns = ['cl_from', 'cl_to', 'cl_to_id']\n",
    "selected_pages = pd.concat([selected_pages, redirect])\n",
    "selected_pages.drop_duplicates().to_csv('./data/page_category.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Events\n",
      "10090\n",
      "processing Health\n",
      "12194\n",
      "processing Culture\n",
      "166300\n",
      "processing World\n",
      "224798\n",
      "processing Games\n",
      "276\n",
      "processing Nature\n",
      "14428\n",
      "processing Humanities\n",
      "29103\n",
      "processing Reference_works\n",
      "751\n",
      "processing Sports\n",
      "51777\n",
      "processing Life\n",
      "15604\n",
      "processing Science_and_technology\n",
      "13825\n",
      "processing History\n",
      "66564\n",
      "processing Philosophy\n",
      "633\n",
      "processing Politics\n",
      "185603\n",
      "processing Law\n",
      "9387\n",
      "processing Mathematics\n",
      "2106\n",
      "processing Matter\n",
      "3595\n",
      "processing Religion\n",
      "10367\n",
      "processing People\n",
      "1637\n",
      "processing Geography\n",
      "49391\n",
      "processing Arts\n",
      "12835\n",
      "processing Society\n",
      "14358\n"
     ]
    }
   ],
   "source": [
    "digraph = nx.read_gpickle('./data/category_network.nx.pickle').reverse(False)\n",
    "\n",
    "labels = nx.get_node_attributes(digraph, 'label')\n",
    "subgraphs = list()\n",
    "for root_node in digraph.neighbors('Main_topic_classifications'):\n",
    "    print(\"processing %s\" % root_node)\n",
    "    _subgraph = digraph.subgraph([x for (x,y) in labels.items() if y == labels[root_node]])\n",
    "    print(_subgraph.number_of_nodes())\n",
    "    # child -> parent\n",
    "    _subtree = pd.DataFrame(np.asarray([[y,x] for (x,y) in nx.bfs_tree(_subgraph, root_node, False).edges()]), columns=['cl_from', 'cl_to'])\n",
    "    # node -> depth\n",
    "    _subdepth = pd.DataFrame(np.asarray([[x,y + 1] for (x,y) in nx.algorithms.shortest_paths.unweighted.single_source_shortest_path_length(_subgraph, root_node, cutoff=100).items()]), columns=['title', 'depth'])\n",
    "    \n",
    "    _subedges = _subtree.merge(_subdepth, left_on='cl_from', right_on='title')[['cl_from', 'cl_to', 'depth']]\n",
    "    _subedges.depth = _subedges.depth.astype(int)\n",
    "    _subedges['label'] = root_node\n",
    "    subgraphs.append(_subedges)\n",
    "    subgraphs.append(pd.DataFrame([[root_node, 'Main_topic_classifications', 1, root_node]], columns=['cl_from', 'cl_to', 'depth', 'label']))\n",
    "subtree = pd.concat(subgraphs)\n",
    "category_id_pd = pd.read_csv('./data/content_category_id.txt', sep=' ')\n",
    "subtree = subtree.merge(category_id_pd, left_on='cl_from', right_on='title')[['cl_from','title_id', 'cl_to', 'depth', 'label']]\n",
    "subtree.columns = ['cl_from', 'cl_from_id', 'cl_to', 'depth', 'label']\n",
    "subtree = subtree.merge(category_id_pd, left_on='cl_to', right_on='title')[['cl_from','cl_from_id', 'cl_to', 'title_id', 'depth', 'label']]\n",
    "subtree.columns = ['cl_from', 'cl_from_id', 'cl_to', 'cl_to_id', 'depth', 'label']\n",
    "\n",
    "# Save graph into files\n",
    "for max_depth in range(1, subtree.depth.max() + 1):\n",
    "    subtree[subtree.depth <= max_depth][['cl_from_id', 'cl_to_id', 'label', 'depth']].to_csv('./data/category_tree.d%d.txt'%max_depth, sep=' ', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
